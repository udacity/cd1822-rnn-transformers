{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abhiojha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhiojha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abhiojha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abhiojha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO:\n",
    "1. Download pretrained models - [Fasttext](https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip) and [Glove](https://nlp.stanford.edu/data/glove.6B.zip)\n",
    "2. Unzip and place the downloaded files (*only* `glove.6B.300d.text` and `wiki-news-300d-1M.vec`) in the `data` directory at the root of this repo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "sample_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = sample_text.lower().split()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GloVE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.3712e-01 -2.1691e-01 -6.6365e-03 -4.1625e-01 -1.2555e+00 -2.8466e-02\n",
      " -7.2195e-01 -5.2887e-01  7.2085e-03  3.1997e-01  2.9425e-02 -1.3236e-02\n",
      "  4.3511e-01  2.5716e-01  3.8995e-01 -1.1968e-01  1.5035e-01  4.4762e-01\n",
      "  2.8407e-01  4.9339e-01  6.2826e-01  2.2888e-01 -4.0385e-01  2.7364e-02\n",
      "  7.3679e-03  1.3995e-01  2.3346e-01  6.8122e-02  4.8422e-01 -1.9578e-02\n",
      " -5.4751e-01 -5.4983e-01 -3.4091e-02  8.0017e-03 -4.3065e-01 -1.8969e-02\n",
      " -8.5670e-02 -8.1123e-01 -2.1080e-01  3.7784e-01 -3.5046e-01  1.3684e-01\n",
      " -5.5661e-01  1.6835e-01 -2.2952e-01 -1.6184e-01  6.7345e-01 -4.6597e-01\n",
      " -3.1834e-02 -2.6037e-01 -1.7797e-01  1.9436e-02  1.0727e-01  6.6534e-01\n",
      " -3.4836e-01  4.7833e-02  1.6440e-01  1.4088e-01  1.9204e-01 -3.5009e-01\n",
      "  2.6236e-01  1.7626e-01 -3.1367e-01  1.1709e-01  2.0378e-01  6.1775e-01\n",
      "  4.9075e-01 -7.5210e-02 -1.1815e-01  1.8685e-01  4.0679e-01  2.8319e-01\n",
      " -1.6290e-01  3.8388e-02  4.3794e-01  8.8224e-02  5.9046e-01 -5.3515e-02\n",
      "  3.8819e-02  1.8202e-01 -2.7599e-01  3.9474e-01 -2.0499e-01  1.7411e-01\n",
      "  1.0315e-01  2.5117e-01 -3.6542e-01  3.6528e-01  2.2448e-01 -9.7551e-01\n",
      "  9.4505e-02 -1.7859e-01 -3.0688e-01 -5.8633e-01 -1.8526e-01  3.9565e-02\n",
      " -4.2309e-01 -1.5715e-01  2.0401e-01  1.6906e-01  3.4465e-01 -4.2262e-01\n",
      "  1.9553e-01  5.9454e-01 -3.0531e-01 -1.0633e-01 -1.9055e-01 -5.8544e-01\n",
      "  2.1357e-01  3.8414e-01  9.1499e-02  3.8353e-01  2.9075e-01  2.4519e-02\n",
      "  2.8440e-01  6.3715e-02 -1.5483e-01  4.0031e-01  3.1543e-01 -3.7128e-02\n",
      "  6.3363e-02 -2.7090e-01  2.5160e-01  4.7105e-01  4.9556e-01 -3.6401e-01\n",
      "  1.0370e-01  4.6076e-02  1.6565e-01 -2.9024e-01 -6.6949e-02 -3.0881e-01\n",
      "  4.8263e-01  3.0972e-01 -1.1145e-01 -1.0329e-01  2.8585e-02 -1.3579e-01\n",
      "  5.2924e-01 -1.4077e-01  9.1763e-02  1.3127e-01 -2.0944e-01  2.2327e-02\n",
      " -7.7692e-02  7.7934e-02 -3.3067e-02  1.1680e-01  3.2029e-01  3.7749e-01\n",
      " -7.5679e-01 -1.5944e-01  1.4964e-01  4.2253e-01  2.8136e-03  2.1328e-01\n",
      "  8.6776e-02 -5.2704e-02 -4.0859e-01 -1.1774e-01  9.0621e-02 -2.3794e-01\n",
      " -1.8326e-01  1.3115e-01 -5.5949e-01  9.2071e-02 -3.9504e-02  1.3334e-01\n",
      "  4.9632e-01  2.8733e-01 -1.8544e-01  2.4618e-02 -4.2826e-01  7.4148e-02\n",
      "  7.6584e-04  2.3950e-01  2.2615e-01  5.5166e-02 -7.5096e-02 -2.2308e-01\n",
      "  2.3775e-01 -4.5455e-01  2.6564e-01 -1.5137e-01 -2.4146e-01 -2.4736e-01\n",
      "  5.5214e-01  2.6819e-01  4.8831e-01 -1.3423e-01 -1.5918e-01  3.7606e-01\n",
      " -1.9834e-01  1.6699e-01 -1.5368e-01  2.4561e-01 -9.2506e-02 -3.0257e-01\n",
      " -2.9493e-01 -7.4917e-01  1.0567e+00  3.7971e-01  6.9314e-01 -3.1672e-02\n",
      "  2.1588e-01 -4.0739e-01 -1.5264e-01  3.2296e-01 -1.2999e-01 -5.0129e-01\n",
      " -4.4231e-01  1.6904e-02 -1.1459e-02  7.2293e-03  1.1026e-01  2.1568e-01\n",
      " -3.2373e-01 -3.7292e-01 -9.2456e-03 -2.6769e-01  3.9066e-01  3.5742e-01\n",
      " -6.0632e-02  6.7966e-02  3.3830e-01  6.5747e-02  1.5794e-01  4.7155e-02\n",
      "  2.3682e-01 -9.1370e-02  6.4649e-01 -2.5491e-01 -6.7940e-01 -6.9752e-01\n",
      " -1.0145e-01 -3.6255e-01  3.6967e-01 -4.1295e-01  8.2724e-02 -3.5053e-01\n",
      " -1.7564e-01  8.5095e-02 -5.7724e-01  5.0252e-01  5.2180e-01  5.7327e-02\n",
      " -7.9754e-01 -3.7770e-01  7.8149e-01  2.4597e-01  6.0672e-01 -2.0082e-01\n",
      " -3.8792e-01  4.1295e-01 -1.6143e-01  1.0427e-02  4.3197e-01  4.6297e-03\n",
      "  2.1185e-01 -2.6606e-01 -5.8740e-02 -5.1003e-01  2.8524e-01  1.3627e-02\n",
      " -2.7346e-01  6.1848e-02 -5.7901e-01 -5.1136e-01  3.6382e-01  3.5144e-01\n",
      " -1.6501e-01 -4.6041e-01 -6.4742e-02 -6.8310e-01 -4.7427e-02  1.5861e-01\n",
      " -4.7288e-01  3.3968e-01  1.2092e-03  1.6018e-01 -5.8024e-01  1.4556e-01\n",
      " -9.1317e-01 -3.7592e-01 -3.2950e-01  5.3465e-01  1.8224e-01 -5.2265e-01\n",
      " -2.6209e-01 -4.2458e-01 -1.8034e-01  9.9502e-02 -1.5114e-01 -6.6731e-01\n",
      "  2.4483e-01 -5.6630e-01  3.3843e-01  4.0558e-01  1.8073e-01  6.4250e-01]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text: str) -> list:\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = ''.join(c for c in text if c not in '.,;:-')\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def load_glove_model(file) -> dict:\n",
    "    # init an empty dict to store \"word\" as key and its \"embedding\" as value.\n",
    "    glove_model = {}\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_model[word] = embedding\n",
    "    return glove_model\n",
    "\n",
    "embedding_dict = load_glove_model(\"../data/glove.6B.300d.txt\")\n",
    "\n",
    "# Let's check embeddings of a word\n",
    "hello_embedding = embedding_dict['hello']\n",
    "print(hello_embedding)\n",
    "# Let's print the embedding vector dimension\n",
    "# This should be 300 as we are using the pretrained model, which generates 300 dim embedding vector\n",
    "print(hello_embedding.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown\n",
      "[ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the embedding matrix for sample_text\n",
    "sample_tokens = preprocess_text(sample_text)\n",
    "sample_embedding_matrix = []\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "    sample_embedding_matrix.append(embedding_dict[sample_token])\n",
    "\n",
    "# we should have as many embedding vectors (rows of embedding matrix) as there are sample tokens\n",
    "assert len(sample_embedding_matrix) == len(sample_tokens)\n",
    "\n",
    "# lets print a token and its embedding\n",
    "print(sample_tokens[2])\n",
    "print(sample_embedding_matrix[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def load_fasttext_model(file) -> dict:\n",
    "    # init an empty dict to store \"word\" as key and its \"embedding\" as value.\n",
    "    fasttext_model = {}\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            fasttext_model[word] = embedding\n",
    "    return fasttext_model\n",
    "\n",
    "fasttext_embeddings_dict = load_fasttext_model(\"../data/wiki-news-300d-1M.vec\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n",
      "[-0.0083  0.0266  0.0112  0.0168 -0.0291 -0.034   0.0204 -0.074  -0.1905\n",
      " -0.0266  0.0245 -0.0254  0.0589  0.0443 -0.0458 -0.0519  0.0587  0.1551\n",
      " -0.1766 -0.0956 -0.0185 -0.0804 -0.0007 -0.1643  0.0288 -0.0548  0.0263\n",
      "  0.0897  0.0194 -0.1682 -0.0025  0.0461 -0.0981 -0.0895  0.0057  0.0592\n",
      " -0.0685 -0.0009  0.0894 -0.0738  0.008   0.0338 -0.114   0.0358 -0.0308\n",
      "  0.0174  0.0246 -0.023   0.0236  0.0314 -0.0347 -0.2517 -0.6334  0.0026\n",
      "  0.0807  0.0234 -0.0055  0.0734 -0.0869 -0.0892 -0.0552 -0.0022  0.0213\n",
      "  0.0716 -0.0678 -0.0334  0.0464 -0.077  -0.0407 -0.0212  0.0162  0.0318\n",
      " -0.0227 -0.1448 -0.0689 -0.0721  0.0393  0.0293 -0.0636 -0.0816  0.0161\n",
      "  0.0443  0.0326 -0.2373  0.0244  0.0431 -0.0376  0.0537  0.041   0.0221\n",
      "  0.0108 -0.0856 -0.0308 -0.1633 -0.1061  0.0288 -0.0396 -0.0036  0.0098\n",
      " -0.1006 -0.1569 -0.0825  0.0698  0.0929 -0.3181 -0.0673  0.3534  0.033\n",
      "  0.0156  0.0464  0.053   0.1033 -0.1109  0.0439 -0.0069 -0.0607  0.0438\n",
      "  0.0525 -0.0186 -0.1776 -0.0716 -0.0673  0.0776  0.1784 -0.0655  0.189\n",
      " -0.0769  0.14   -0.1617 -0.0808 -0.0652  0.0819 -0.0531 -0.0043 -0.0696\n",
      " -0.2738 -0.0362  0.0689  0.1458 -0.0342  0.041  -0.0456  0.0294  0.2467\n",
      "  0.0403  0.0126  0.0786 -0.0125  0.1215 -0.0178  0.2816  0.0954  0.0367\n",
      "  0.0245  0.0273  0.0398  0.0513 -0.1115 -0.0702 -0.0324 -0.0352  0.0393\n",
      "  0.0423 -0.0305 -0.3716 -0.0255  0.0546  0.1251  0.0219 -0.0133 -0.0096\n",
      "  0.058   0.0932 -0.01   -0.0291 -0.1296  0.2119 -0.2818  0.0043 -0.0187\n",
      "  0.0468  0.0069  0.0186  0.0618  0.0963 -0.1248 -0.0382 -0.1108  0.2328\n",
      " -0.0356  0.0158 -0.0345 -0.0072 -0.0304  0.0749  0.0185 -0.0685 -0.0117\n",
      "  0.2267  0.0447  0.1537  0.136  -0.0303 -0.0516  0.1063 -0.1074  0.11\n",
      " -0.164   0.0412  0.007  -0.1451  0.0448 -0.0266 -0.0216 -0.0021 -0.0169\n",
      " -0.1013 -0.004   0.0452 -0.0389  0.0512 -0.0169  0.0371  0.0306 -0.0162\n",
      " -0.0531  0.0231 -0.0451 -0.0041  0.1424 -0.0337 -0.0698  0.2498 -0.0709\n",
      "  0.0158  0.0945 -0.0776  0.047  -0.2119  0.0135 -0.0477 -0.0011  0.0291\n",
      " -0.0436  0.0162  0.024  -0.1025  0.0051 -0.0344  0.3393  0.0683 -0.0186\n",
      "  0.0251 -0.0872 -0.144   0.0092  0.0553  0.0661 -0.0184 -0.0604  0.0057\n",
      "  0.0532  0.0228 -0.0283 -0.072  -0.0411  0.1401 -0.0118  0.0339  0.0694\n",
      " -0.0205  0.0164 -0.0065 -0.0581 -0.041  -0.0186  0.1365  0.0848 -0.0938\n",
      " -0.0626  0.0664  0.0317  0.0732 -0.0218  0.0185  0.0075 -0.0164  0.0667\n",
      " -0.1395  0.0833 -0.0138  0.0085  0.0057  0.0157 -0.0141 -0.0126 -0.0175\n",
      "  0.2633 -0.0017 -0.0479]\n"
     ]
    }
   ],
   "source": [
    "# Now let's create the embedding matrix for sample_text\n",
    "sample_tokens = preprocess_text(sample_text)\n",
    "sample_embedding_matrix = []\n",
    "\n",
    "for sample_token in sample_tokens:\n",
    "    sample_embedding_matrix.append(fasttext_embeddings_dict[sample_token])\n",
    "\n",
    "# we should have as many embedding vectors (rows of embedding matrix) as there are sample tokens\n",
    "assert len(sample_embedding_matrix) == len(sample_tokens)\n",
    "\n",
    "# lets print a token and its embedding\n",
    "print(sample_tokens[5])\n",
    "print(sample_embedding_matrix[5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
